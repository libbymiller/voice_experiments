{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning experiments\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"uk_garden_birds_mock\"\n",
    "path = \"20200827_uk_garden_birds_mock/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the models\n",
    "\n",
    "Use the model previously created (see naturewatch_pytorch_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded full model\n"
     ]
    }
   ],
   "source": [
    "## unpruned model\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "with open(path+model_id+'_classes.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    class_list = list(reader)[0]\n",
    "\n",
    "# we need the correct structure to load it (I think?)\n",
    "model_full = models.resnet34() #load resnet structure\n",
    "num_ftrs = model_full.fc.in_features \n",
    "num_classes = len(class_list)\n",
    "model_full.fc = nn.Linear(num_ftrs, num_classes) #change final layer\n",
    "\n",
    "model_full.load_state_dict(torch.load(path+model_id+'_model.pt', map_location=map_location))\n",
    "model_full.eval()\n",
    "print(\"loaded full model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions stuff\n",
    "\n",
    "def transform_image(image_bytes): #change to match trainging\n",
    "    my_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            [0.485, 0.456, 0.406],\n",
    "                                            [0.229, 0.224, 0.225])])\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    return my_transforms(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "def get_predictions(image_bytes, n, model0):\n",
    "    tensor = transform_image(image_bytes=image_bytes)\n",
    "    outputs = model0(tensor)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    probs = softmax(outputs)\n",
    "    \n",
    "    top_probs, top_idxs = torch.topk(probs, n)\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        print(class_list[top_idxs[0][i]], top_probs[0][i].item())\n",
    "        \n",
    "def time_test_prediction(path, model0):\n",
    "    with open(path, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "        get_predictions(image_bytes=image_bytes, n=3, model0=model0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224 # must be 224 for Resnet default\n",
    "batch_size = 32\n",
    "weight_decay = 0.01  # PyTorch default = 0, fastai says to try 0.1\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),   # augmentation\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.Resize((int(image_size*1.1),int(image_size*1.1))),       # upscale if required\n",
    "    transforms.CenterCrop((image_size,image_size)),   # resize\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size,image_size)),       # upscale if required\n",
    "    transforms.CenterCrop((image_size,image_size)),   # resize\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "class DatasetFromCSV(Dataset):\n",
    "    def __init__(self, path_csv, path_img_dir, transform=None, classes_list = None):\n",
    "        df_fromCSV = pd.read_csv(path_csv, header=None)\n",
    "        self.img_paths = df_fromCSV[0]\n",
    "        self.img_labels = self.convert_label_strings_to_indices(df_fromCSV[1], classes_list) if classes_list is not None else df_fromCSV[1] \n",
    "        self.path_csv = path_csv\n",
    "        self.path_img_dir = path_img_dir\n",
    "        self.transform = transform\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.to_pil = ToPILImage()\n",
    "        self.image_size = image_size\n",
    "    \n",
    "    def get_image_from_folder(self, name):\n",
    "        img = Image.open(os.path.join(self.path_img_dir, name))\n",
    "        if img.mode == 'CMYK':\n",
    "            img = img.convert('RGB')\n",
    "        return img \n",
    "    \n",
    "    def convert_label_strings_to_indices(self, labels_as_strings, classes):\n",
    "        labels_as_indices = []\n",
    "        classes = classes.tolist()\n",
    "        for label in labels_as_strings:\n",
    "            labels_as_indices.append(int(classes.index(label)))\n",
    "        return labels_as_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ''' \n",
    "        returns image as normalised tensors of shape (num_channels, num_px_x, num_px_y)\n",
    "        returns label as index (i.e. of type int) relative to its position in the classes array\n",
    "        '''\n",
    "        image = self.get_image_from_folder(self.img_paths[index])\n",
    "        try:\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "        except RuntimeError as rerr:\n",
    "            print (rerr, self.img_paths[index], index)\n",
    "        label = self.img_labels[index] \n",
    "\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def __getrandom__(self):\n",
    "        '''\n",
    "        like getitem but gets a _random_ image (path) and its label\n",
    "        '''\n",
    "        index = random.randint(0, len(self.img_paths))\n",
    "        ##image = self.get_image_from_folder(self.img_paths[index])\n",
    "        label = self.img_labels[index]\n",
    "        \n",
    "        return self.img_paths[index], label\n",
    "\n",
    "# load subset from CSV\n",
    "basePaths_csvs = \"20201107_balanced_test_and_validation/lists/\" # <--- adjust to your system\n",
    "path_CSV_test =  basePaths_csvs  + \"uk_garden_birds_balanced_test.csv\"\n",
    "basePath_ds = \"uk_garden_birds_balanced/\"\n",
    "image_path_test = basePath_ds + \"test\"\n",
    "num_workers = 0 # processing cores to use\n",
    "\n",
    "classes = pd.read_csv(path_CSV_test, header=None)[1].unique() \n",
    "\n",
    "data_test = DatasetFromCSV(path_CSV_test, image_path_test, test_transform, classes)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=batch_size,\n",
    "     num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, classes):\n",
    "    # track test loss\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(classes.size)) # create emtpy array with slot for each class\n",
    "    class_total = list(0. for i in range(classes.size))\n",
    "\n",
    "    accuracy = []\n",
    "    imagenum = []\n",
    "\n",
    "    #track predictions and targets for evaluation\n",
    "    predslist=torch.zeros(0,dtype=torch.long, device=map_location)\n",
    "    imageslist=torch.zeros([0,3, image_size, image_size], device=map_location) #keep track of the order of the batches' images \n",
    "    targslist=torch.zeros(0,dtype=torch.long, device=map_location)\n",
    "    losseslist=torch.zeros(0,dtype=torch.float, device=map_location)\n",
    "    probslist=torch.zeros(0,dtype=torch.float, device=map_location)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # outputs mean squared error for the overall batch\n",
    "    criterion_perSample = nn.CrossEntropyLoss(reduction = 'none')\n",
    "\n",
    "    model.eval()\n",
    "    train_on_gpu = False\n",
    "\n",
    "    print (\"Evaluating model performance on test set ... \\n\")\n",
    "    # iterate over test data\n",
    "    for data, target in test_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target) # mse batch loss \n",
    "        losses_perSample = criterion_perSample(output, target) # individual losses per image sample\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "        # normalize output to probabalites adding to 1 with softmax \n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probs = softmax(output)\n",
    "    \n",
    "        # add predictions and targets to list\n",
    "        predslist=torch.cat([predslist,pred.view(-1).cpu()])\n",
    "        imageslist= torch.cat([imageslist,data.data.cpu()])\n",
    "        targslist=torch.cat([targslist,target.view(-1).cpu()])\n",
    "        losseslist=torch.cat([losseslist, losses_perSample.data.cpu()])\n",
    "        probslist=torch.cat([probslist, probs.data.cpu()])\n",
    "    \n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # calculate test accuracy for each object class\n",
    "    \n",
    "        for i in range(target.data.size()[0]): # for every sample in current batch (last batch might not be full batchsize if n_X % batch_size != 0)\n",
    "            label = target.data[i]\n",
    "            try:\n",
    "                class_correct[label] += correct[i].item()\n",
    "            except:\n",
    "                class_correct[label] += correct\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "         100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model performance on test set ... \n",
      "\n",
      "Test Loss: 1.739588\n",
      "\n",
      "Test Accuracy (Overall): 54% (757/1400)\n"
     ]
    }
   ],
   "source": [
    "test_model(model_full, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Sparrow 0.13860872387886047\n",
      "Eurasian Sparrowhawk 0.12889806926250458\n",
      "Long-tailed Tit 0.12249056994915009\n",
      "CPU times: user 136 ms, sys: 18.3 ms, total: 155 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p = 'test/Eurasian Sparrowhawk/15bccfd131deb7f7334771e1d3771944.jpg'\n",
    "time_test_prediction(p, model_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 1\n",
    "\n",
    "pruning_method=prune.L1Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded prune 1 model\n",
      "Evaluating model performance on test set ... \n",
      "\n",
      "Test Loss: 2.079975\n",
      "\n",
      "Test Accuracy (Overall): 46% (656/1400)\n"
     ]
    }
   ],
   "source": [
    "## prune 1 - \n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "with open(path+model_id+'_classes.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    class_list = list(reader)[0]\n",
    "\n",
    "# we need the correct structure to load it (I think?)\n",
    "model_prune1 = models.resnet34() #load resnet structure\n",
    "num_ftrs = model_prune1.fc.in_features \n",
    "num_classes = len(class_list)\n",
    "model_prune1.fc = nn.Linear(num_ftrs, num_classes) #change final layer\n",
    "\n",
    "model_prune1.load_state_dict(torch.load(path+model_id+'_model.pt', map_location=map_location))\n",
    "model_prune1.eval()\n",
    "print(\"loaded prune 1 model\")\n",
    "\n",
    "#https://discuss.pytorch.org/t/module-children-vs-module-modules/4551\n",
    "#parameters_to_prune = [(child, \"weight\") for child in model.modules()]\n",
    "parameters_to_prune = [\n",
    "    (child, \"weight\")\n",
    "    for child in model_prune1.modules()\n",
    "    if (isinstance(child, torch.nn.Conv2d) or isinstance(child, torch.nn.BatchNorm2d))\n",
    "]\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.5,\n",
    ")\n",
    "test_model(model_prune1, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Sparrow 0.02619250863790512\n",
      "European Starling 0.021677931770682335\n",
      "Mallard 0.020930804312229156\n",
      "CPU times: user 162 ms, sys: 25.9 ms, total: 187 ms\n",
      "Wall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p = 'test/Eurasian Sparrowhawk/15bccfd131deb7f7334771e1d3771944.jpg'\n",
    "time_test_prediction(p, model_prune1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 2\n",
    "\n",
    "test pruning by weights theshold\n",
    "https://stackoverflow.com/questions/61629395/how-to-prune-weights-less-than-a-threshold-in-pytorch\n",
    "\n",
    "see some more stuff here https://stackoverflow.com/questions/62326683/prunning-model-doesnt-improve-inference-speed-or-reduce-model-size\n",
    "\n",
    "it may not improve inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded prune 2 model\n",
      "Evaluating model performance on test set ... \n",
      "\n",
      "Test Loss: 1.739588\n",
      "\n",
      "Test Accuracy (Overall): 54% (757/1400)\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "# reset dosn't actually reset. need to load it all again\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "with open(path+model_id+'_classes.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    class_list = list(reader)[0]\n",
    "\n",
    "# we need the correct structure to load it (I think?)\n",
    "model_prune2 = models.resnet34() #load resnet structure\n",
    "num_ftrs = model_prune2.fc.in_features \n",
    "num_classes = len(class_list)\n",
    "model_prune2.fc = nn.Linear(num_ftrs, num_classes) #change final layer\n",
    "\n",
    "model_prune2.load_state_dict(torch.load(path+model_id+'_model.pt', map_location=map_location))\n",
    "model_prune2.eval()\n",
    "\n",
    "print(\"loaded prune 2 model\")\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        #print(\"self.threshold...\",self.threshold)\n",
    "        #print(\"tttt\",torch.abs(tensor) > self.threshold)\n",
    "        return torch.abs(tensor) > self.threshold\n",
    "    \n",
    "# weights seem to be be between 1 and 2\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune, pruning_method=ThresholdPruning, threshold=1.1\n",
    ")\n",
    "test_model(model_prune2, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mallard 0.06670695543289185\n",
      "Common Wood Pigeon 0.04748813435435295\n",
      "Coal Tit 0.032256465405225754\n",
      "CPU times: user 197 ms, sys: 55.7 ms, total: 253 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p = 'test/Eurasian Sparrowhawk/15bccfd131deb7f7334771e1d3771944.jpg'\n",
    "time_test_prediction(p, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
